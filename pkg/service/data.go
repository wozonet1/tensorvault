package service

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"errors"
	"fmt"
	"io"

	tvrpc "tensorvault/pkg/api/tvrpc/v1"
	"tensorvault/pkg/app"
	"tensorvault/pkg/ingester"
	"tensorvault/pkg/storage"
	"tensorvault/pkg/types"

	"google.golang.org/grpc"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/status"
)

type DataService struct {
	tvrpc.UnimplementedDataServiceServer
	app *app.App
}

func NewDataService(application *app.App) *DataService {
	return &DataService{
		app: application,
	}
}

// =============================================================================
// 0. Pre-check (Optimistic Deduplication)
// =============================================================================

// CheckFile å®ç°äº†â€œåŒé˜¶æ®µä¸Šä¼ â€çš„ç¬¬ä¸€é˜¶æ®µ
// å®¢æˆ·ç«¯æä¾›æ–‡ä»¶çš„ LinearHash å’Œ Sizeï¼ŒæœåŠ¡ç«¯æ£€æŸ¥æ˜¯å¦å·²æœ‰å¯¹åº”ç´¢å¼•
func (s *DataService) CheckFile(ctx context.Context, req *tvrpc.CheckFileRequest) (*tvrpc.CheckFileResponse, error) {
	// 1. å‚æ•°è½¬æ¢ä¸æ ¡éªŒ
	linearHash := types.LinearHash(req.Sha256)
	if !linearHash.IsValid() {
		// å°½ç®¡ Proto æœ‰ validateï¼Œè¿™é‡Œæ˜¯æœ€åä¸€é“é˜²çº¿
		return nil, status.Error(codes.InvalidArgument, "invalid sha256 format")
	}

	// 2. æŸ¥è¯¢å…ƒæ•°æ®ç´¢å¼•
	// s.app.Repository æ˜¯æˆ‘ä»¬åœ¨ Step 2 ä¸­å¢å¼ºè¿‡çš„
	idx, err := s.app.Repository.GetFileIndex(ctx, linearHash)
	if err != nil {
		// æ•°æ®åº“æŸ¥è¯¢å‡ºé”™ (Connection Refused ç­‰) -> è¿”å› Internal Error
		return nil, status.Errorf(codes.Internal, "failed to query file index: %v", err)
	}

	// 3. Cache Miss (æ•°æ®åº“é‡Œæ²¡æŸ¥åˆ°)
	if idx == nil {
		return &tvrpc.CheckFileResponse{
			Exists: false,
		}, nil
	}

	// 4. å®‰å…¨å…œåº•ï¼šå“ˆå¸Œç¢°æ’æ£€æµ‹
	// å¦‚æœ Hash ä¸€æ ·ä½† Size ä¸ä¸€æ ·ï¼Œè¯´æ˜å‘ç”Ÿç¢°æ’ï¼ˆæˆ–è€…æ•°æ®åº“è„æ•°æ®ï¼‰
	// è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ä¸æ•¢å¤ç”¨ï¼Œå¼ºåˆ¶å®¢æˆ·ç«¯é‡æ–°ä¸Šä¼ 
	if idx.SizeBytes != req.Size {
		fmt.Printf("âš ï¸ Hash Collision or Corruption detected! Hash: %s, DB Size: %d, Req Size: %d\n",
			linearHash, idx.SizeBytes, req.Size)
		return &tvrpc.CheckFileResponse{
			Exists: false, // æ¬ºéª—å®¢æˆ·ç«¯è¯´ä¸å­˜åœ¨ï¼Œå¼ºåˆ¶é‡ä¼ 
		}, nil
	}

	// 5. å†æ¬¡ç¡®è®¤åº•å±‚å¯¹è±¡å­˜åœ¨ (Double Check)
	// è™½ç„¶ç´¢å¼•è¡¨é‡Œæœ‰è®°å½•ï¼Œä½†ä¸‡ä¸€ S3 é‡Œçš„å¯¹è±¡è¢«è¯¯åˆ äº†å‘¢ï¼Ÿ
	// æˆ‘ä»¬åšä¸€ä¸ªå¿«é€Ÿçš„ Has æ£€æŸ¥ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±ã€‚
	exists, err := s.app.Store.Has(ctx, idx.MerkleRoot)
	if err != nil {
		// S3 æŠ¥é”™ï¼Œå®‰å…¨èµ·è§è®©å®¢æˆ·ç«¯é‡ä¼ 
		return nil, status.Errorf(codes.Internal, "storage check failed: %v", err)
	}
	if !exists {
		fmt.Printf("âš ï¸ Data Integrity Alert: Index exists for %s but FileNode %s is missing in store.\n",
			linearHash, idx.MerkleRoot)
		// ç´¢å¼•æ‚¬ç©ºï¼Œéœ€è¦é‡ä¼ 
		return &tvrpc.CheckFileResponse{Exists: false}, nil
	}

	// 6. Cache Hit (ç§’ä¼ æˆåŠŸ)
	fmt.Printf("âš¡ [CheckFile] Instant upload for %s (Hash: %s)\n", linearHash[:8], idx.MerkleRoot[:8])

	// è¿™é‡Œéœ€è¦å¤„ç† optional å­—æ®µçš„èµ‹å€¼
	// proto3 optional å¯¹åº” Go çš„æŒ‡é’ˆç±»å‹ *string
	rootHashStr := idx.MerkleRoot.String()
	return &tvrpc.CheckFileResponse{
		Exists:         true,
		MerkleRootHash: &rootHashStr,
	}, nil
}

// =============================================================================
// 1. Upload (Client-Side Streaming) with Integrity Check & Indexing
// =============================================================================

// Upload æ¥æ”¶å®¢æˆ·ç«¯çš„æµå¼ä¸Šä¼ 
// åè®®çº¦å®šï¼šç¬¬ä¸€å¸§å¿…é¡»æ˜¯ Meta (å« sha256)ï¼Œåç»­å¸§æ˜¯ ChunkData
func (s *DataService) Upload(stream grpc.ClientStreamingServer[tvrpc.UploadRequest, tvrpc.UploadResponse]) error {
	// --- Step 1: æ¡æ‰‹é˜¶æ®µ (Handshake) ---
	firstReq, err := stream.Recv()
	if err == io.EOF {
		return status.Error(codes.InvalidArgument, "empty stream: expected metadata frame")
	}
	if err != nil {
		return status.Errorf(codes.Internal, "failed to receive metadata: %v", err)
	}

	meta := firstReq.GetMeta()
	if meta == nil {
		return status.Error(codes.InvalidArgument, "protocol violation: first frame must be FileMeta")
	}

	// æ ¡éªŒ Meta ä¸­çš„ Linear Hash æ˜¯å¦åˆæ³•
	clientLinearHash := types.LinearHash(meta.Sha256)
	if !clientLinearHash.IsValid() {
		return status.Errorf(codes.InvalidArgument, "invalid sha256 in metadata: %s", meta.Sha256)
	}

	fmt.Printf("ğŸš€ [Upload] Receiving: %s (Claimed Hash: %s...)\n", meta.Path, clientLinearHash[:8])

	// --- Step 2: ç»„è£…é˜¶æ®µ (Wiring) ---
	// 1. gRPC Stream -> io.Reader
	streamReader := NewGrpcStreamReader(stream)

	// 2. å‡†å¤‡ SHA-256 Hasher (ç”¨äºæœåŠ¡ç«¯ç«¯è®¡ç®—å…¨é‡å“ˆå¸Œ)
	hasher := sha256.New()

	// 3. ç»„è£… TeeReader: è¯» streamReader çš„åŒæ—¶ï¼Œè‡ªåŠ¨å†™å…¥ hasher
	teeReader := io.TeeReader(streamReader, hasher)

	// 4. åˆ›å»º Ingester
	ing := ingester.NewIngester(s.app.Store)

	// --- Step 3: æ‰§è¡Œé˜¶æ®µ (Execution) ---
	// Ingester è¯»å– teeReader -> è§¦å‘ Hasher è®¡ç®— -> è§¦å‘ CDC åˆ‡åˆ† -> ä¸Šä¼  S3
	ctx := stream.Context()
	fileNode, err := ing.IngestFile(ctx, teeReader)
	if err != nil {
		return status.Errorf(codes.Internal, "ingestion failed: %v", err)
	}

	// --- Step 4: å®Œæ•´æ€§æ ¡éªŒ (Integrity Check) ---
	// æ­¤æ—¶æµå·²è¯»å®Œï¼ŒHasher ä¸­å·²ç»æœ‰äº†å…¨é‡æ•°æ®çš„æŒ‡çº¹
	serverLinearHashStr := hex.EncodeToString(hasher.Sum(nil))
	serverLinearHash := types.LinearHash(serverLinearHashStr)

	if serverLinearHash != clientLinearHash {
		// è¿™æ˜¯ä¸€ä¸ªä¸¥é‡é”™è¯¯ï¼šæ•°æ®åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­æŸåï¼Œæˆ–è€…å®¢æˆ·ç«¯æ’’è°äº†
		// å³ä½¿ S3 å·²ç»å­˜äº†æ•°æ®ï¼Œæˆ‘ä»¬ä¹Ÿä¸èƒ½è®¤é¢†å®ƒï¼ˆå®ƒæ˜¯è„æ•°æ®ï¼‰
		fmt.Printf("âŒ [Upload] Integrity Check Failed!\nClaimed: %s\nActual : %s\n", clientLinearHash, serverLinearHash)
		return status.Errorf(codes.DataLoss, "integrity check failed: data corruption detected")
	}

	// --- Step 5: å»ºç«‹ç´¢å¼• (Indexing) ---
	// æ ¡éªŒé€šè¿‡ï¼Œè¯´æ˜ S3 é‡Œçš„æ•°æ®æ˜¯å®Œå¥½ä¸”æ­£ç¡®çš„ã€‚
	// ç°åœ¨æˆ‘ä»¬å°† LinearHash -> MerkleRoot çš„å…³ç³»å†™å…¥æ•°æ®åº“ï¼Œä¾›ä¸‹æ¬¡ CheckFile ä½¿ç”¨ã€‚
	err = s.app.Repository.SaveFileIndex(ctx, serverLinearHash, fileNode.ID(), fileNode.TotalSize)
	if err != nil {
		// ç´¢å¼•å†™å…¥å¤±è´¥ä¸åº”å½±å“ä¸Šä¼ æˆåŠŸçš„åˆ¤å®šï¼ˆå±äºéå…³é”®è·¯å¾„å¤±è´¥ï¼‰
		// ä½†ä¸ºäº†ç³»ç»Ÿå¥åº·ï¼Œæˆ‘ä»¬éœ€è¦è®°å½•æ—¥å¿—
		fmt.Printf("âš ï¸ [Upload] Failed to save file index: %v\n", err)
		// é€‰æ‹©ï¼šæ˜¯æŠ¥é”™è¿˜æ˜¯å¿½ç•¥ï¼Ÿ
		// æ¶æ„å†³ç­–ï¼šå¿½ç•¥é”™è¯¯ã€‚æ–‡ä»¶å·²ç»å®‰å…¨å­˜å…¥ S3 å¹¶è¿”å›äº† Hashï¼Œç”¨æˆ·å¯ä»¥ç»§ç»­å·¥ä½œã€‚
		// åªæ˜¯ä¸‹æ¬¡æ²¡æ³•â€œç§’ä¼ â€è€Œå·²ã€‚è¿™æ˜¯â€œå¯ç”¨æ€§ä¼˜å…ˆâ€ã€‚
	} else {
		fmt.Printf("âœ… [Upload] Index saved. Linear: %s -> Merkle: %s\n", serverLinearHash[:8], fileNode.ID()[:8])
	}

	// --- Step 6: å“åº”é˜¶æ®µ (Response) ---
	return stream.SendAndClose(&tvrpc.UploadResponse{
		Hash:      fileNode.ID().String(),
		TotalSize: fileNode.TotalSize,
	})
}

// =============================================================================
// 2. Download (Server-Side Streaming)
// =============================================================================

// Download å¤„ç†ä¸‹è½½è¯·æ±‚
func (s *DataService) Download(req *tvrpc.DownloadRequest, stream grpc.ServerStreamingServer[tvrpc.DownloadResponse]) error {
	// --- Step 1: å‚æ•°æ ¡éªŒ ---
	ctx := stream.Context()
	inputHash := req.Hash

	// [ä¿®æ”¹] æ™ºèƒ½å“ˆå¸Œè§£æï¼šæ”¯æŒå®Œæ•´å“ˆå¸Œå’ŒçŸ­å“ˆå¸Œ
	var targetHash types.Hash

	if len(inputHash) == 64 {
		// 1. å¦‚æœæ˜¯å®Œæ•´å“ˆå¸Œï¼Œç›´æ¥ä½¿ç”¨ (æ€§èƒ½æœ€ä¼˜)
		targetHash = types.Hash(inputHash)
	} else {
		// 2. å¦‚æœæ˜¯çŸ­å“ˆå¸Œï¼Œå°è¯•æ‰©å±• (ç”¨æˆ·å‹å¥½)
		// æ³¨æ„ï¼šExpandHash æ˜¯ Store æ¥å£çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬åœ¨ Phase 1 å·²ç»å®ç°äº†
		fullHash, err := s.app.Store.ExpandHash(ctx, types.HashPrefix(inputHash))
		if err != nil {
			if errors.Is(err, storage.ErrNotFound) {
				return status.Errorf(codes.NotFound, "hash prefix %s not found", inputHash)
			}
			if errors.Is(err, storage.ErrAmbiguousHash) {
				return status.Errorf(codes.InvalidArgument, "hash prefix %s is ambiguous", inputHash)
			}
			return status.Errorf(codes.Internal, "hash expansion failed: %v", err)
		}
		targetHash = fullHash
	}

	fmt.Printf("ğŸ“¦ [Download] Serving: %s (Expanded from: %s)\n", targetHash, inputHash)

	// --- Step 2: ç»„è£…é€‚é…å™¨ ---
	// æŠŠ gRPC stream ä¼ªè£…æˆ io.Writer
	streamWriter := NewGrpcStreamWriter(stream)

	// --- Step 3: æ‰§è¡Œå¯¼å‡º ---
	// åˆ›å»º Exporter
	exp := s.app.GetExporter() // ç¨åè¦åœ¨ App é‡ŒåŠ è¿™ä¸ª helper æ–¹æ³•ï¼Œæˆ–è€…ç›´æ¥ new

	// è°ƒç”¨æ ¸å¿ƒé€»è¾‘
	// æ³¨æ„ï¼šExporter å†…éƒ¨ä¼šæ£€æµ‹ streamWriter æ˜¯å¦æ”¯æŒ WriteAtã€‚
	// æ˜¾ç„¶ GrpcStreamWriter ä¸æ”¯æŒï¼Œæ‰€ä»¥ Exporter ä¼šè‡ªåŠ¨é™çº§ä¸ºä¸²è¡Œæµå¼ä¼ è¾“ï¼Œ
	// è¿™æ­£æ˜¯ gRPC Server Streaming æ‰€éœ€è¦çš„æ¨¡å¼ã€‚
	err := exp.ExportFile(stream.Context(), targetHash, streamWriter)

	// --- Step 4: é”™è¯¯å¤„ç† ---
	if err != nil {
		// æ˜ å°„æ ¸å¿ƒå±‚é”™è¯¯åˆ° gRPC çŠ¶æ€ç 
		if errors.Is(err, storage.ErrNotFound) {
			return status.Errorf(codes.NotFound, "object %s not found", targetHash)
		}
		return status.Errorf(codes.Internal, "export failed: %v", err)
	}

	return nil
}
